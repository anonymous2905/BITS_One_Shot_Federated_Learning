{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f71b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained base model.\n",
      "\n",
      "Starting Federated Round 1\n",
      "Client 1 training...\n",
      "Client 2 training...\n",
      "Client 3 training...\n",
      "Client 4 training...\n",
      "Client 5 training...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Evaluation Metrics:\n",
      "Accuracy : 0.9100\n",
      "Precision: 0.9118\n",
      "Recall   : 0.9100\n",
      "F1 Score : 0.9099\n",
      "Saved fine-tuned global model as 'non_iid_vgg16_fedprox_updated.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "latent_dim = 50\n",
    "num_clients = 5\n",
    "epochs_per_client = 10\n",
    "mu = 0.01  # FedProx proximal term coefficient\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Dataset paths\n",
    "base_path = \"C:\\\\Users\\\\paray\\\\OneDrive\\\\Desktop\\\\bits new research\\\\dataset\\\\dataset\\\\organized_screenshots\"\n",
    "categories = {\n",
    "    0: os.path.join(base_path, \"Education\", \"Coursera\"),\n",
    "    1: os.path.join(base_path, \"Education\", \"Programming\"),\n",
    "    2: os.path.join(base_path, \"Education\", \"YouTube\"),\n",
    "    3: os.path.join(base_path, \"Entertainment\", \"YouTube\"),\n",
    "    4: os.path.join(base_path, \"Shopping\"),\n",
    "}\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_data_limited_per_class(categories, max_per_class=100):\n",
    "    X, y = [], []\n",
    "    for label, folder in categories.items():\n",
    "        count = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder, file)\n",
    "                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                    img = img.resize(image_size)\n",
    "                    img_array = np.array(img) / 255.0\n",
    "                    X.append(img_array)\n",
    "                    y.append(label)\n",
    "                    count += 1\n",
    "                    if count >= max_per_class:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file}: {e}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_data_limited_per_class(categories, max_per_class=100)\n",
    "y_cat = to_categorical(y, num_classes=5)\n",
    "\n",
    "# Train-test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y_cat, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Create non-IID clients\n",
    "def create_noniid_clients(X, y, num_clients=5):\n",
    "    data = list(zip(X, y))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    class_buckets = {i: [] for i in range(5)}\n",
    "    for xi, yi in data:\n",
    "        class_idx = np.argmax(yi)\n",
    "        class_buckets[class_idx].append((xi, yi))\n",
    "\n",
    "    clients_data = [[] for _ in range(num_clients)]\n",
    "\n",
    "    for cls in class_buckets:\n",
    "        cls_data = class_buckets[cls]\n",
    "        split_size = len(cls_data) // num_clients\n",
    "        for i in range(num_clients):\n",
    "            start = i * split_size\n",
    "            end = start + split_size if i < num_clients - 1 else len(cls_data)\n",
    "            clients_data[i].extend(cls_data[start:end])\n",
    "\n",
    "    X_clients, y_clients = [], []\n",
    "    for client_data in clients_data:\n",
    "        random.shuffle(client_data)\n",
    "        Xc, yc = zip(*client_data)\n",
    "        X_clients.append(np.array(Xc))\n",
    "        y_clients.append(np.array(yc))\n",
    "\n",
    "    return X_clients, y_clients\n",
    "\n",
    "# Build base VGG16 model\n",
    "def build_fedprox_vgg16(input_shape=(224, 224, 3), num_classes=5):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Custom FedProx training per client\n",
    "def train_fedprox_model(local_model, global_trainable_weights, X, y, mu, epochs):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(1000).batch(batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step, (x_batch, y_batch) in enumerate(dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = local_model(x_batch, training=True)\n",
    "                loss = loss_fn(y_batch, preds)\n",
    "\n",
    "                # FedProx proximal term\n",
    "                prox_term = 0.0\n",
    "                for w_local, w_global in zip(local_model.trainable_weights, global_trainable_weights):\n",
    "                    prox_term += tf.reduce_sum(tf.square(w_local - w_global))\n",
    "                loss += (mu / 2.0) * prox_term\n",
    "\n",
    "            grads = tape.gradient(loss, local_model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, local_model.trainable_weights))\n",
    "\n",
    "# Federated training with FedProx\n",
    "def federated_fedprox_training(model, X_clients, y_clients, epochs, mu):\n",
    "    global_weights = model.get_weights()\n",
    "    global_trainable_weights = model.trainable_weights\n",
    "    global_trainable_values = [w.numpy() for w in global_trainable_weights]\n",
    "\n",
    "    client_weights = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        print(f\"Client {i+1} training...\")\n",
    "        local_model = build_fedprox_vgg16()\n",
    "        local_model.set_weights(global_weights)\n",
    "        train_fedprox_model(local_model, global_trainable_values, X_clients[i], y_clients[i], mu, epochs)\n",
    "        client_weights.append(local_model.get_weights())\n",
    "\n",
    "    new_weights = []\n",
    "    for weights in zip(*client_weights):\n",
    "        new_weights.append(np.mean(weights, axis=0))\n",
    "\n",
    "    return new_weights\n",
    "\n",
    "# Load pretrained IID-trained base model\n",
    "pretrained_model_path = r\"C:\\\\Users\\\\paray\\\\OneDrive\\\\Desktop\\\\bits new research\\\\final_trained_vgg16_model.h5\"\n",
    "global_model = load_model(pretrained_model_path)\n",
    "print(\"Loaded pretrained base model.\")\n",
    "\n",
    "# Create non-IID client datasets\n",
    "X_clients, y_clients = create_noniid_clients(X_train_full, y_train_full, num_clients)\n",
    "\n",
    "# Federated training (1 round)\n",
    "print(\"\\nStarting Federated Round 1\")\n",
    "updated_weights = federated_fedprox_training(global_model, X_clients, y_clients, epochs=epochs_per_client, mu=mu)\n",
    "global_model.set_weights(updated_weights)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_probs = global_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\nFinal Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "# Save model\n",
    "global_model.save(\"non_iid_vgg16_fedprox_updated.h5\")\n",
    "print(\"Saved fine-tuned global model as 'non_iid_vgg16_fedprox_updated.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbeaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded pretrained VGG19 model.\n",
      "\n",
      "üöÄ Starting Federated Training Round\n",
      "\n",
      "üîß Training Client 1\n",
      "\n",
      "üîß Training Client 2\n",
      "\n",
      "üîß Training Client 3\n",
      "\n",
      "üîß Training Client 4\n",
      "\n",
      "üîß Training Client 5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4s/step - accuracy: 0.9397 - loss: 0.2944\n",
      "\n",
      "üéØ Final Global Model Test Accuracy (Keras): 0.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Classification Metrics:\n",
      "üîπ Accuracy:  0.9300\n",
      "üîπ Precision: 0.9312\n",
      "üîπ Recall:    0.9300\n",
      "üîπ F1-score:  0.9294\n",
      "üíæ Saved updated model to 'non_iid_vgg19_fedprox_updated.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ----------------- CONFIGURATION -----------------\n",
    "latent_dim = 50\n",
    "num_clients = 5\n",
    "epochs_per_client = 30\n",
    "mu = 0.01\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "num_classes = 5\n",
    "max_images_per_class = 100\n",
    "\n",
    "base_path = \"C:\\\\Users\\\\paray\\\\OneDrive\\\\Desktop\\\\bits new research\\\\dataset\\\\dataset\\\\organized_screenshots\"\n",
    "categories = {\n",
    "    0: os.path.join(base_path, \"Education\", \"Coursera\"),\n",
    "    1: os.path.join(base_path, \"Education\", \"Programming\"),\n",
    "    2: os.path.join(base_path, \"Education\", \"YouTube\"),\n",
    "    3: os.path.join(base_path, \"Entertainment\", \"YouTube\"),\n",
    "    4: os.path.join(base_path, \"Shopping\"),\n",
    "}\n",
    "\n",
    "# ----------------- DATA LOADING -----------------\n",
    "def load_data(categories, max_per_class):\n",
    "    X, y = [], []\n",
    "    for label, folder in categories.items():\n",
    "        count = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if file.lower().endswith(('.jpg', '.png')):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder, file)\n",
    "                    img = Image.open(img_path).convert(\"RGB\").resize(image_size)\n",
    "                    X.append(np.array(img) / 255.0)\n",
    "                    y.append(label)\n",
    "                    count += 1\n",
    "                    if count >= max_per_class:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Skipping {file}: {e}\")\n",
    "    return np.array(X), to_categorical(np.array(y), num_classes=num_classes)\n",
    "\n",
    "X, y_cat = load_data(categories, max_images_per_class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_cat, test_size=0.2, stratify=np.argmax(y_cat, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# ----------------- CLIENT SPLITTING -----------------\n",
    "def create_noniid_clients(X, y, num_clients):\n",
    "    data = list(zip(X, y))\n",
    "    random.shuffle(data)\n",
    "    class_buckets = {i: [] for i in range(num_classes)}\n",
    "    for xi, yi in data:\n",
    "        class_idx = np.argmax(yi)\n",
    "        class_buckets[class_idx].append((xi, yi))\n",
    "    clients = [[] for _ in range(num_clients)]\n",
    "    for cls_data in class_buckets.values():\n",
    "        random.shuffle(cls_data)\n",
    "        split_size = len(cls_data) // num_clients\n",
    "        for i in range(num_clients):\n",
    "            start = i * split_size\n",
    "            end = len(cls_data) if i == num_clients - 1 else (i + 1) * split_size\n",
    "            clients[i].extend(cls_data[start:end])\n",
    "    X_clients, y_clients = [], []\n",
    "    for data in clients:\n",
    "        random.shuffle(data)\n",
    "        X_i, y_i = zip(*data)\n",
    "        X_clients.append(np.array(X_i))\n",
    "        y_clients.append(np.array(y_i))\n",
    "    return X_clients, y_clients\n",
    "\n",
    "X_clients, y_clients = create_noniid_clients(X_train, y_train, num_clients)\n",
    "\n",
    "# ----------------- MODEL DEFINITION -----------------\n",
    "def build_vgg19_model(input_shape=(224, 224, 3), num_classes=5, trainable=False):\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = trainable\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# ----------------- FEDPROX LOCAL TRAINING -----------------\n",
    "def train_fedprox_model(local_model, global_weights, X, y, mu, epochs):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(1000).batch(batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = local_model(x_batch, training=True)\n",
    "                loss = loss_fn(y_batch, preds)\n",
    "                # Proximal term\n",
    "                prox_term = 0.0\n",
    "                for w, w_t in zip(local_model.trainable_weights, global_weights):\n",
    "                    prox_term += tf.reduce_sum(tf.square(w - w_t))\n",
    "                loss += (mu / 2.0) * prox_term\n",
    "            grads = tape.gradient(loss, local_model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, local_model.trainable_weights))\n",
    "\n",
    "# ----------------- FEDPROX AGGREGATION -----------------\n",
    "def federated_fedprox_training(global_model, X_clients, y_clients, epochs, mu):\n",
    "    global_weights = [w.numpy() for w in global_model.trainable_weights]\n",
    "    client_weights = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        print(f\"\\nüîß Training Client {i + 1}\")\n",
    "        local_model = build_vgg19_model()\n",
    "        local_model.set_weights(global_model.get_weights())\n",
    "        train_fedprox_model(local_model, global_weights, X_clients[i], y_clients[i], mu, epochs)\n",
    "        client_weights.append(local_model.get_weights())\n",
    "\n",
    "    # FedAvg aggregation\n",
    "    new_weights = [np.mean(w, axis=0) for w in zip(*client_weights)]\n",
    "    return new_weights\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    pretrained_model_path = r\"C:\\Users\\paray\\OneDrive\\Desktop\\bits new research\\final_global_vgg19_model.h5\"\n",
    "    global_model = load_model(pretrained_model_path)\n",
    "    print(\"‚úÖ Loaded pretrained VGG19 model.\")\n",
    "\n",
    "    # Federated training round\n",
    "    print(\"\\nüöÄ Starting Federated Training Round\")\n",
    "    updated_weights = federated_fedprox_training(global_model, X_clients, y_clients, epochs_per_client, mu)\n",
    "    global_model.set_weights(updated_weights)\n",
    "\n",
    "    # Evaluation\n",
    "    global_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss, acc = global_model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"\\nüéØ Final Global Model Test Accuracy (Keras): {acc:.4f}\")\n",
    "\n",
    "    # Custom metrics\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = np.argmax(global_model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(\"\\nüìä Classification Metrics:\")\n",
    "    print(f\"üîπ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"üîπ Precision: {precision:.4f}\")\n",
    "    print(f\"üîπ Recall:    {recall:.4f}\")\n",
    "    print(f\"üîπ F1-score:  {f1:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    save_path = \"non_iid_vgg19_fedprox_updated.h5\"\n",
    "    global_model.save(save_path)\n",
    "    print(f\"üíæ Saved updated model to '{save_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080483bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded pretrained ResNet50 model.\n",
      "\n",
      "üöÄ Starting Federated Training Round\n",
      "\n",
      "üîß Training Client 1\n",
      "\n",
      "üîß Training Client 2\n",
      "\n",
      "üîß Training Client 3\n",
      "\n",
      "üîß Training Client 4\n",
      "\n",
      "üîß Training Client 5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7041 - loss: 0.8867\n",
      "\n",
      "üéØ Final Global Model Test Accuracy (Keras): 0.6900\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DD6B2E0280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DD6B2E0280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DD6B2E0280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DD6B2E0280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Classification Metrics:\n",
      "üîπ Accuracy:  0.6900\n",
      "üîπ Precision: 0.7813\n",
      "üîπ Recall:    0.6900\n",
      "üîπ F1-score:  0.6878\n",
      "üíæ Saved updated model to 'non_iid_resnet50_fedprox_updated.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ----------------- CONFIGURATION -----------------\n",
    "latent_dim = 50\n",
    "num_clients = 5\n",
    "epochs_per_client = 30\n",
    "mu = 0.01\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "num_classes = 5\n",
    "max_images_per_class = 100\n",
    "\n",
    "# Dataset paths\n",
    "base_path = \"C:\\\\Users\\\\paray\\\\OneDrive\\\\Desktop\\\\bits new research\\\\dataset\\\\dataset\\\\organized_screenshots\"\n",
    "categories = {\n",
    "    0: os.path.join(base_path, \"Education\", \"Coursera\"),\n",
    "    1: os.path.join(base_path, \"Education\", \"Programming\"),\n",
    "    2: os.path.join(base_path, \"Education\", \"YouTube\"),\n",
    "    3: os.path.join(base_path, \"Entertainment\", \"YouTube\"),\n",
    "    4: os.path.join(base_path, \"Shopping\"),\n",
    "}\n",
    "\n",
    "# ----------------- DATA LOADING -----------------\n",
    "def load_data(categories, max_per_class):\n",
    "    X, y = [], []\n",
    "    for label, folder in categories.items():\n",
    "        count = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if file.lower().endswith(('.jpg', '.png')):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder, file)\n",
    "                    img = Image.open(img_path).convert(\"RGB\").resize(image_size)\n",
    "                    X.append(np.array(img) / 255.0)\n",
    "                    y.append(label)\n",
    "                    count += 1\n",
    "                    if count >= max_per_class:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Skipping {file}: {e}\")\n",
    "    return np.array(X), to_categorical(np.array(y), num_classes=num_classes)\n",
    "\n",
    "X, y_cat = load_data(categories, max_images_per_class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_cat, test_size=0.2, stratify=np.argmax(y_cat, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# ----------------- CLIENT SPLITTING -----------------\n",
    "def create_noniid_clients(X, y, num_clients):\n",
    "    data = list(zip(X, y))\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    class_buckets = {i: [] for i in range(num_classes)}\n",
    "    for xi, yi in data:\n",
    "        class_idx = np.argmax(yi)\n",
    "        class_buckets[class_idx].append((xi, yi))\n",
    "\n",
    "    clients = [[] for _ in range(num_clients)]\n",
    "    for cls_data in class_buckets.values():\n",
    "        random.shuffle(cls_data)\n",
    "        split_size = len(cls_data) // num_clients\n",
    "        for i in range(num_clients):\n",
    "            start = i * split_size\n",
    "            end = len(cls_data) if i == num_clients - 1 else (i + 1) * split_size\n",
    "            clients[i].extend(cls_data[start:end])\n",
    "\n",
    "    X_clients, y_clients = [], []\n",
    "    for data in clients:\n",
    "        random.shuffle(data)\n",
    "        X_i, y_i = zip(*data)\n",
    "        X_clients.append(np.array(X_i))\n",
    "        y_clients.append(np.array(y_i))\n",
    "    return X_clients, y_clients\n",
    "\n",
    "X_clients, y_clients = create_noniid_clients(X_train, y_train, num_clients)\n",
    "\n",
    "# ----------------- MODEL DEFINITION -----------------\n",
    "def build_resnet50_model(input_shape=(224, 224, 3), num_classes=5, trainable=False):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = trainable\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# ----------------- FEDPROX TRAINING -----------------\n",
    "def federated_fedprox_training(global_model, X_clients, y_clients, epochs, mu):\n",
    "    global_weights = global_model.get_weights()\n",
    "    client_weights = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        print(f\"\\nüîß Training Client {i + 1}\")\n",
    "        local_model = build_resnet50_model(trainable=False)\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(len(X_clients[i]))\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, len(indices), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_idx = indices[start:end]\n",
    "                x_batch, y_batch = X_clients[i][batch_idx], y_clients[i][batch_idx]\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    predictions = local_model(x_batch, training=True)\n",
    "                    ce_loss = loss_fn(y_batch, predictions)\n",
    "                    \n",
    "                    # FedProx regularization term\n",
    "                    prox_term = 0.0\n",
    "                    for w, w_glob in zip(local_model.trainable_weights, global_weights):\n",
    "                        prox_term += tf.reduce_sum(tf.square(w - w_glob))\n",
    "                    total_loss = ce_loss + (mu / 2) * prox_term\n",
    "\n",
    "                grads = tape.gradient(total_loss, local_model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, local_model.trainable_weights))\n",
    "\n",
    "        client_weights.append(local_model.get_weights())\n",
    "\n",
    "    # FedAvg\n",
    "    new_weights = [np.mean(w, axis=0) for w in zip(*client_weights)]\n",
    "    return new_weights\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    pretrained_model_path = r\"C:\\Users\\paray\\OneDrive\\Desktop\\bits new research\\updated_global_resnet50_model.h5\"\n",
    "    global_model = load_model(pretrained_model_path)\n",
    "    print(\"‚úÖ Loaded pretrained ResNet50 model.\")\n",
    "\n",
    "    # Federated training round\n",
    "    print(\"\\nüöÄ Starting Federated Training Round\")\n",
    "    updated_weights = federated_fedprox_training(global_model, X_clients, y_clients, epochs_per_client, mu)\n",
    "    global_model.set_weights(updated_weights)\n",
    "\n",
    "    # Evaluation\n",
    "    global_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss, acc = global_model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"\\nüéØ Final Global Model Test Accuracy (Keras): {acc:.4f}\")\n",
    "\n",
    "    # Custom metrics\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = np.argmax(global_model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(\"\\nüìä Classification Metrics:\")\n",
    "    print(f\"üîπ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"üîπ Precision: {precision:.4f}\")\n",
    "    print(f\"üîπ Recall:    {recall:.4f}\")\n",
    "    print(f\"üîπ F1-score:  {f1:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    save_path = \"non_iid_resnet50_fedprox_updated.h5\"\n",
    "    global_model.save(save_path)\n",
    "    print(f\"üíæ Saved updated model to '{save_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "289a5304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded pretrained InceptionV3 model.\n",
      "\n",
      "üöÄ Starting Federated Training Round with FedProx\n",
      "\n",
      "üîß Training Client 1\n",
      "\n",
      "üîß Training Client 2\n",
      "\n",
      "üîß Training Client 3\n",
      "\n",
      "üîß Training Client 4\n",
      "\n",
      "üîß Training Client 5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9838 - loss: 0.0457\n",
      "\n",
      "üéØ Final Global Model Test Accuracy: 0.9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Classification Metrics:\n",
      "üîπ Accuracy:  0.9700\n",
      "üîπ Precision: 0.9710\n",
      "üîπ Recall:    0.9700\n",
      "üîπ F1-score:  0.9702\n",
      "üíæ Saved updated model to 'non_iid_inceptionv3_fedprox_updated.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ----------------- CONFIGURATION -----------------\n",
    "latent_dim = 50\n",
    "num_clients = 5\n",
    "epochs_per_client = 30\n",
    "mu = 0.01  # FedProx proximal term coefficient\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "image_size = (299, 299)\n",
    "num_classes = 5\n",
    "max_images_per_class = 100\n",
    "\n",
    "# Dataset paths\n",
    "base_path = \"C:\\\\Users\\\\paray\\\\OneDrive\\\\Desktop\\\\bits new research\\\\dataset\\\\dataset\\\\organized_screenshots\"\n",
    "categories = {\n",
    "    0: os.path.join(base_path, \"Education\", \"Coursera\"),\n",
    "    1: os.path.join(base_path, \"Education\", \"Programming\"),\n",
    "    2: os.path.join(base_path, \"Education\", \"YouTube\"),\n",
    "    3: os.path.join(base_path, \"Entertainment\", \"YouTube\"),\n",
    "    4: os.path.join(base_path, \"Shopping\"),\n",
    "}\n",
    "\n",
    "# ----------------- DATA LOADING -----------------\n",
    "def load_data(categories, max_per_class):\n",
    "    X, y = [], []\n",
    "    for label, folder in categories.items():\n",
    "        count = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if file.lower().endswith(('.jpg', '.png')):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder, file)\n",
    "                    img = Image.open(img_path).convert(\"RGB\").resize(image_size)\n",
    "                    X.append(np.array(img) / 255.0)\n",
    "                    y.append(label)\n",
    "                    count += 1\n",
    "                    if count >= max_per_class:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Skipping {file}: {e}\")\n",
    "    return np.array(X), to_categorical(np.array(y), num_classes=num_classes)\n",
    "\n",
    "X, y_cat = load_data(categories, max_images_per_class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_cat, test_size=0.2, stratify=np.argmax(y_cat, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# ----------------- CLIENT SPLITTING -----------------\n",
    "def create_noniid_clients(X, y, num_clients):\n",
    "    data = list(zip(X, y))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    class_buckets = {i: [] for i in range(num_classes)}\n",
    "    for xi, yi in data:\n",
    "        class_idx = np.argmax(yi)\n",
    "        class_buckets[class_idx].append((xi, yi))\n",
    "\n",
    "    clients = [[] for _ in range(num_clients)]\n",
    "    for cls_data in class_buckets.values():\n",
    "        random.shuffle(cls_data)\n",
    "        split_size = len(cls_data) // num_clients\n",
    "        for i in range(num_clients):\n",
    "            start = i * split_size\n",
    "            end = len(cls_data) if i == num_clients - 1 else (i + 1) * split_size\n",
    "            clients[i].extend(cls_data[start:end])\n",
    "\n",
    "    X_clients, y_clients = [], []\n",
    "    for data in clients:\n",
    "        random.shuffle(data)\n",
    "        X_i, y_i = zip(*data)\n",
    "        X_clients.append(np.array(X_i))\n",
    "        y_clients.append(np.array(y_i))\n",
    "    return X_clients, y_clients\n",
    "\n",
    "X_clients, y_clients = create_noniid_clients(X_train, y_train, num_clients)\n",
    "\n",
    "# ----------------- MODEL DEFINITION -----------------\n",
    "def build_inceptionv3_model(input_shape=(299, 299, 3), num_classes=5, trainable=False):\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = trainable\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ----------------- LOCAL TRAINING WITH FEDPROX -----------------\n",
    "def train_local_fedprox_model(local_model, global_model, X, y, epochs, mu):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Get reference trainable weights from global model directly\n",
    "    global_trainable_weights = [tf.convert_to_tensor(w) for w in global_model.trainable_weights]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(buffer_size=1024).batch(batch_size)\n",
    "        for step, (batch_x, batch_y) in enumerate(dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = local_model(batch_x, training=True)\n",
    "                ce_loss = loss_fn(batch_y, predictions)\n",
    "                # FedProx proximal term (on trainable weights only)\n",
    "                prox_loss = 0.0\n",
    "                for lw, gw in zip(local_model.trainable_weights, global_trainable_weights):\n",
    "                    prox_loss += tf.reduce_sum(tf.square(lw - gw))\n",
    "                total_loss = ce_loss + (mu / 2) * prox_loss\n",
    "            grads = tape.gradient(total_loss, local_model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, local_model.trainable_weights))\n",
    "    return local_model.get_weights()\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- FEDPROX TRAINING -----------------\n",
    "def federated_fedprox_training(global_model, X_clients, y_clients, epochs, mu):\n",
    "    global_weights = global_model.get_weights()\n",
    "    client_weights = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        print(f\"\\nüîß Training Client {i + 1}\")\n",
    "        local_model = build_inceptionv3_model()\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        # üîß Corrected line below\n",
    "        updated_weights = train_local_fedprox_model(local_model, global_model, X_clients[i], y_clients[i], epochs, mu)\n",
    "        client_weights.append(updated_weights)\n",
    "\n",
    "    # FedAvg Aggregation\n",
    "    new_weights = [np.mean(w, axis=0) for w in zip(*client_weights)]\n",
    "    return new_weights\n",
    "\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    pretrained_model_path = r\"C:\\Users\\paray\\OneDrive\\Desktop\\bits new research\\final_global_inceptionv3_model.h5\"\n",
    "    global_model = load_model(pretrained_model_path)\n",
    "    print(\"‚úÖ Loaded pretrained InceptionV3 model.\")\n",
    "\n",
    "    print(\"\\nüöÄ Starting Federated Training Round with FedProx\")\n",
    "    updated_weights = federated_fedprox_training(global_model, X_clients, y_clients, epochs_per_client, mu)\n",
    "    global_model.set_weights(updated_weights)\n",
    "\n",
    "    global_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss, acc = global_model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"\\nüéØ Final Global Model Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Classification metrics\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = np.argmax(global_model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(\"\\nüìä Classification Metrics:\")\n",
    "    print(f\"üîπ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"üîπ Precision: {precision:.4f}\")\n",
    "    print(f\"üîπ Recall:    {recall:.4f}\")\n",
    "    print(f\"üîπ F1-score:  {f1:.4f}\")\n",
    "\n",
    "    save_path = \"non_iid_inceptionv3_fedprox_updated.h5\"\n",
    "    global_model.save(save_path)\n",
    "    print(f\"üíæ Saved updated model to '{save_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f200004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model weights into FedInceptionResNetV2 model.\n",
      "\n",
      "üöÄ Starting Federated Training Round\n",
      "\n",
      "üîß Training Client 1\n",
      "\n",
      "üîß Training Client 2\n",
      "\n",
      "üîß Training Client 3\n",
      "\n",
      "üîß Training Client 4\n",
      "\n",
      "üîß Training Client 5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.9746 - loss: 0.1144\n",
      "\n",
      "üéØ Final Global Model Test Accuracy (Keras): 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Classification Metrics:\n",
      "üîπ Accuracy:  0.9600\n",
      "üîπ Precision: 0.9614\n",
      "üîπ Recall:    0.9600\n",
      "üîπ F1-score:  0.9600\n",
      "üíæ Saved updated model to 'non_iid_inceptionresnetv2_fedprox_updated.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ----------------- CONFIGURATION -----------------\n",
    "latent_dim = 50\n",
    "num_clients = 5\n",
    "epochs_per_client = 30\n",
    "mu = 0.01\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "num_classes = 5\n",
    "max_images_per_class = 100\n",
    "\n",
    "# Dataset paths\n",
    "base_path = \"C:\\\\Users\\\\paray\\\\OneDrive\\\\Desktop\\\\bits new research\\\\dataset\\\\dataset\\\\organized_screenshots\"\n",
    "categories = {\n",
    "    0: os.path.join(base_path, \"Education\", \"Coursera\"),\n",
    "    1: os.path.join(base_path, \"Education\", \"Programming\"),\n",
    "    2: os.path.join(base_path, \"Education\", \"YouTube\"),\n",
    "    3: os.path.join(base_path, \"Entertainment\", \"YouTube\"),\n",
    "    4: os.path.join(base_path, \"Shopping\"),\n",
    "}\n",
    "\n",
    "# ----------------- DATA LOADING -----------------\n",
    "def load_data(categories, max_per_class):\n",
    "    X, y = [], []\n",
    "    for label, folder in categories.items():\n",
    "        count = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if file.lower().endswith(('.jpg', '.png')):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder, file)\n",
    "                    img = Image.open(img_path).convert(\"RGB\").resize(image_size)\n",
    "                    X.append(np.array(img) / 255.0)\n",
    "                    y.append(label)\n",
    "                    count += 1\n",
    "                    if count >= max_per_class:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Skipping {file}: {e}\")\n",
    "    return np.array(X), to_categorical(np.array(y), num_classes=num_classes)\n",
    "\n",
    "X, y_cat = load_data(categories, max_images_per_class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_cat, test_size=0.2, stratify=np.argmax(y_cat, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# ----------------- CLIENT SPLITTING -----------------\n",
    "def create_noniid_clients(X, y, num_clients):\n",
    "    data = list(zip(X, y))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    class_buckets = {i: [] for i in range(num_classes)}\n",
    "    for xi, yi in data:\n",
    "        class_idx = np.argmax(yi)\n",
    "        class_buckets[class_idx].append((xi, yi))\n",
    "\n",
    "    clients = [[] for _ in range(num_clients)]\n",
    "    for cls_data in class_buckets.values():\n",
    "        random.shuffle(cls_data)\n",
    "        split_size = len(cls_data) // num_clients\n",
    "        for i in range(num_clients):\n",
    "            start = i * split_size\n",
    "            end = len(cls_data) if i == num_clients - 1 else (i + 1) * split_size\n",
    "            clients[i].extend(cls_data[start:end])\n",
    "\n",
    "    X_clients, y_clients = [], []\n",
    "    for data in clients:\n",
    "        random.shuffle(data)\n",
    "        X_i, y_i = zip(*data)\n",
    "        X_clients.append(np.array(X_i))\n",
    "        y_clients.append(np.array(y_i))\n",
    "    return X_clients, y_clients\n",
    "\n",
    "X_clients, y_clients = create_noniid_clients(X_train, y_train, num_clients)\n",
    "\n",
    "# ----------------- MODEL DEFINITION (InceptionResNetV2) -----------------\n",
    "def FedInceptionResNetV2(input_shape=(224, 224, 3), num_classes=5):\n",
    "    base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# ----------------- FEDPROX LOSS FUNCTION -----------------\n",
    "def fedprox_loss(global_weights, local_model, mu):\n",
    "    # Extract only trainable global weights\n",
    "    global_trainable_weights = [\n",
    "        w for w, lw in zip(global_weights, local_model.weights) if lw.trainable\n",
    "    ]\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        prox_term = 0.0\n",
    "        for w, w_t in zip(local_model.trainable_weights, global_trainable_weights):\n",
    "            prox_term += tf.reduce_sum(tf.square(w - tf.convert_to_tensor(w_t)))\n",
    "        return base_loss + (mu / 2) * prox_term\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "# ----------------- FEDPROX TRAINING -----------------\n",
    "def federated_fedprox_training(global_model, X_clients, y_clients, epochs, mu):\n",
    "    global_weights = global_model.get_weights()\n",
    "    client_weights = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        print(f\"\\nüîß Training Client {i + 1}\")\n",
    "        local_model = FedInceptionResNetV2()\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        local_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "            loss=fedprox_loss(global_weights, local_model, mu),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        local_model.fit(\n",
    "            X_clients[i], y_clients[i],\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        client_weights.append(local_model.get_weights())\n",
    "\n",
    "    # FedAvg aggregation\n",
    "    new_weights = [np.mean(w, axis=0) for w in zip(*client_weights)]\n",
    "    return new_weights\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    pretrained_model_path = r\"C:\\Users\\paray\\OneDrive\\Desktop\\bits new research\\updated_global_inceptionresnetv2_model.h5\"\n",
    "    global_model = FedInceptionResNetV2()\n",
    "    global_model.load_weights(pretrained_model_path)\n",
    "    print(\"‚úÖ Loaded model weights into FedInceptionResNetV2 model.\")\n",
    "\n",
    "    print(\"\\nüöÄ Starting Federated Training Round\")\n",
    "    updated_weights = federated_fedprox_training(global_model, X_clients, y_clients, epochs_per_client, mu)\n",
    "    global_model.set_weights(updated_weights)\n",
    "\n",
    "    # Evaluation\n",
    "    global_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss, acc = global_model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"\\nüéØ Final Global Model Test Accuracy (Keras): {acc:.4f}\")\n",
    "\n",
    "    # Custom metrics\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = np.argmax(global_model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(\"\\nüìä Classification Metrics:\")\n",
    "    print(f\"üîπ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"üîπ Precision: {precision:.4f}\")\n",
    "    print(f\"üîπ Recall:    {recall:.4f}\")\n",
    "    print(f\"üîπ F1-score:  {f1:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    save_path = \"non_iid_inceptionresnetv2_fedprox_updated.h5\"\n",
    "    global_model.save(save_path)\n",
    "    print(f\"üíæ Saved updated model to '{save_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf606414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model weights into FedMobileNet model.\n",
      "\n",
      "üöÄ Starting Federated Training Round (MobileNet + FedProx)\n",
      "\n",
      "üîß Training Client 1\n",
      "\n",
      "üîß Training Client 2\n",
      "\n",
      "üîß Training Client 3\n",
      "\n",
      "üîß Training Client 4\n",
      "\n",
      "üîß Training Client 5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 607ms/step - accuracy: 0.9868 - loss: 0.0264\n",
      "\n",
      "üéØ Final Global Model Test Accuracy (MobileNet): 0.9800\n",
      "\n",
      "üìä Classification Metrics:\n",
      "üîπ Accuracy:  0.9800\n",
      "üîπ Precision: 0.9810\n",
      "üîπ Recall:    0.9800\n",
      "üîπ F1 Score:  0.9797\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ----------------- CONFIGURATION -----------------\n",
    "latent_dim = 50\n",
    "num_clients = 5\n",
    "epochs_per_client = 30\n",
    "mu = 0.01\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "num_classes = 5\n",
    "max_images_per_class = 100\n",
    "\n",
    "# Dataset paths\n",
    "base_path = \"C:\\\\Users\\\\paray\\\\OneDrive\\\\Desktop\\\\bits new research\\\\dataset\\\\dataset\\\\organized_screenshots\"\n",
    "categories = {\n",
    "    0: os.path.join(base_path, \"Education\", \"Coursera\"),\n",
    "    1: os.path.join(base_path, \"Education\", \"Programming\"),\n",
    "    2: os.path.join(base_path, \"Education\", \"YouTube\"),\n",
    "    3: os.path.join(base_path, \"Entertainment\", \"YouTube\"),\n",
    "    4: os.path.join(base_path, \"Shopping\"),\n",
    "}\n",
    "\n",
    "# ----------------- DATA LOADING -----------------\n",
    "def load_data(categories, max_per_class):\n",
    "    X, y = [], []\n",
    "    for label, folder in categories.items():\n",
    "        count = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if file.lower().endswith(('.jpg', '.png')):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder, file)\n",
    "                    img = Image.open(img_path).convert(\"RGB\").resize(image_size)\n",
    "                    X.append(np.array(img) / 255.0)\n",
    "                    y.append(label)\n",
    "                    count += 1\n",
    "                    if count >= max_per_class:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Skipping {file}: {e}\")\n",
    "    return np.array(X), to_categorical(np.array(y), num_classes=num_classes)\n",
    "\n",
    "X, y_cat = load_data(categories, max_images_per_class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_cat, test_size=0.2, stratify=np.argmax(y_cat, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# ----------------- CLIENT SPLITTING -----------------\n",
    "def create_noniid_clients(X, y, num_clients):\n",
    "    data = list(zip(X, y))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    class_buckets = {i: [] for i in range(num_classes)}\n",
    "    for xi, yi in data:\n",
    "        class_idx = np.argmax(yi)\n",
    "        class_buckets[class_idx].append((xi, yi))\n",
    "\n",
    "    clients = [[] for _ in range(num_clients)]\n",
    "    for cls_data in class_buckets.values():\n",
    "        random.shuffle(cls_data)\n",
    "        split_size = len(cls_data) // num_clients\n",
    "        for i in range(num_clients):\n",
    "            start = i * split_size\n",
    "            end = len(cls_data) if i == num_clients - 1 else (i + 1) * split_size\n",
    "            clients[i].extend(cls_data[start:end])\n",
    "\n",
    "    X_clients, y_clients = [], []\n",
    "    for data in clients:\n",
    "        random.shuffle(data)\n",
    "        X_i, y_i = zip(*data)\n",
    "        X_clients.append(np.array(X_i))\n",
    "        y_clients.append(np.array(y_i))\n",
    "    return X_clients, y_clients\n",
    "\n",
    "X_clients, y_clients = create_noniid_clients(X_train, y_train, num_clients)\n",
    "\n",
    "# ----------------- MODEL DEFINITION (MobileNet) -----------------\n",
    "def FedMobileNet(input_shape=(224, 224, 3), num_classes=5):\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# ----------------- FEDPROX LOSS FUNCTION -----------------\n",
    "def fedprox_loss(global_weights, local_model, mu):\n",
    "    global_trainable_weights = [\n",
    "        w for w, lw in zip(global_weights, local_model.weights) if lw.trainable\n",
    "    ]\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        base_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        prox_term = 0.0\n",
    "        for w, w_t in zip(local_model.trainable_weights, global_trainable_weights):\n",
    "            prox_term += tf.reduce_sum(tf.square(w - tf.convert_to_tensor(w_t)))\n",
    "        return base_loss + (mu / 2) * prox_term\n",
    "    return loss_fn\n",
    "\n",
    "# ----------------- FEDPROX TRAINING -----------------\n",
    "def federated_fedprox_training(global_model, X_clients, y_clients, epochs, mu):\n",
    "    global_weights = global_model.get_weights()\n",
    "    client_weights = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        print(f\"\\nüîß Training Client {i + 1}\")\n",
    "        local_model = FedMobileNet()\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        local_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "            loss=fedprox_loss(global_weights, local_model, mu),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        local_model.fit(\n",
    "            X_clients[i], y_clients[i],\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        client_weights.append(local_model.get_weights())\n",
    "\n",
    "    # FedAvg aggregation\n",
    "    new_weights = [np.mean(w, axis=0) for w in zip(*client_weights)]\n",
    "    return new_weights\n",
    "\n",
    "# ----------------- MAIN SCRIPT -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    pretrained_model_path = r\"C:\\Users\\paray\\OneDrive\\Desktop\\bits new research\\updated_mobilenet_model.h5\"\n",
    "    global_model = FedMobileNet()\n",
    "    global_model.load_weights(pretrained_model_path)\n",
    "    print(\"‚úÖ Loaded model weights into FedMobileNet model.\")\n",
    "\n",
    "    print(\"\\nüöÄ Starting Federated Training Round (MobileNet + FedProx)\")\n",
    "    updated_weights = federated_fedprox_training(global_model, X_clients, y_clients, epochs_per_client, mu)\n",
    "    global_model.set_weights(updated_weights)\n",
    "\n",
    "    # Evaluation\n",
    "    global_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss, acc = global_model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"\\nüéØ Final Global Model Test Accuracy (MobileNet): {acc:.4f}\")\n",
    "\n",
    "    # Custom metrics\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = np.argmax(global_model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(\"\\nüìä Classification Metrics:\")\n",
    "    print(f\"üîπ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"üîπ Precision: {precision:.4f}\")\n",
    "    print(f\"üîπ Recall:    {recall:.4f}\")\n",
    "    print(f\"üîπ F1 Score:  {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
